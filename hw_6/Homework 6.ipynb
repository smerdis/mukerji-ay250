{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 6: Homebrew Computer vision\n",
    "\n",
    "I didn't have time to refactor this, so it's a standard \"run-the-cells in order\" notebook with explanations and answers to questions inline in Markdown cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup, exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os, glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from skimage import io\n",
    "\n",
    "from skimage.filters import gabor\n",
    "from skimage.feature import hog, canny\n",
    "\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The categories are:  ['gorilla', 'raccoon', 'crab', 'blimp', 'snail', 'airplanes', 'dog', 'dolphin', 'goldfish', 'giraffe', 'bear', 'killer-whale', 'penguin', 'zebra', 'duck', 'conch', 'camel', 'owl', 'helicopter', 'starfish', 'saturn', 'galaxy', 'goat', 'iguana', 'elk', 'hummingbird', 'triceratops', 'porcupine', 'teddy-bear', 'comet', 'hot-air-balloon', 'leopards', 'toad', 'mussels', 'kangaroo', 'speed-boat', 'bat', 'swan', 'octopus', 'frog', 'cormorant', 'unicorn', 'horse', 'skunk', 'mars', 'ostrich', 'goose', 'llama', 'snake', 'elephant']\n"
     ]
    }
   ],
   "source": [
    "all_categories = [os.path.split(path)[-1] for path in glob.glob(\"50_categories/*\")]\n",
    "print(\"The categories are: \", all_categories)\n",
    "all_filenames = glob.glob(\"50_categories/*/*.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust next line to train on all or a subset of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset_filenames = all_filenames[::5]\n",
    "n_files_to_classify = len(subset_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_image(filename):\n",
    "    img = io.imread(filename)\n",
    "    #print(f\"Image shape: {img.shape}\")\n",
    "    return img\n",
    "\n",
    "def split_into_rgb_channels(image):\n",
    "    r = image[:,:,0]\n",
    "    g = image[:,:,1]\n",
    "    b = image[:,:,2]\n",
    "    return r, g, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions that take an image and return a feature\n",
    "\n",
    "Ok, so let's talk about features. I iterated a few times here but ultimately didn't get features I'm happy with. The stupid/obvious features brought most of the performance boost, and my \"smarter\" features didn't really seem to help that much, despite taking a bunch of computational and programmer time. I think this is because I'm condensing the smarter features down to into single numbers that throw away most of the useful information... but I did this because I was looking for a balance between complexity and speed, and thought a few useful statistics of well-known image transform like e.g. gabor filters would do as well as classifying based on the transformed image itself. This seems not to be true, and I'm out of time to build something better.\n",
    "\n",
    "If I had more time, here's the approach I would take: First, refactor this whole thing so that it's object-oriented, and small stuff like handling multichannel vs grayscale images, efficient application of multiple functions to a transformed image without recomputing, features of different sizes all getting neatly flattened, etc, is all handled. Also, eliminate the ugly code repetition across channels and features. You can see I made some progress here towards computing e.g. a gabor filter once, then doing several things to it, returning all those. That way the filter only needs to be done once and you can get many features out of it. Then, add some more features using additional functions of scikit-image. Finally, use the filtered images themselves as features, rather than computing single numbers from them. I could have added multiprocessing here too. I think ultimately I need a much bigger diversity of features than I have. Oh well, I learned a lot at least!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_imsize(img):\n",
    "    return img.shape[:2]\n",
    "\n",
    "def feat_chanratio(img, c1, c2):\n",
    "    return np.mean(img[:,:,c1])/np.mean(img[:,:,c2]) if len(img.shape) > 2 else 1\n",
    "\n",
    "def feat_quadrant(img, channel, func=np.max):\n",
    "    h, w = img.shape[:2]\n",
    "    half_h = h//2\n",
    "    half_w = w//2\n",
    "    ic = img[:,:,channel] if len(img.shape) > 2 else img\n",
    "    qs = {1: func(ic[:half_h,:half_w]),\n",
    "         2: func(ic[half_h:,:half_w]),\n",
    "         3: func(ic[:half_h,half_w:]),\n",
    "         4: func(ic[half_h:,half_w:])}\n",
    "    return max(qs, key=lambda key: qs[key])\n",
    "\n",
    "def feat_histogram(img, channel, funcs=[stats.kurtosis]):\n",
    "    ic = img[:,:,channel] if len(img.shape) > 2 else img\n",
    "    h, fd = np.histogram(ic)\n",
    "    #print(ic.shape, h.shape, h, fd.shape, fd)\n",
    "    return [f(h) for f in funcs]\n",
    "    \n",
    "# smarter features\n",
    "def feat_gabor(img, channel, f, funcs=[np.max]):\n",
    "    ic = img[:,:,channel] if len(img.shape) > 2 else img\n",
    "    f_real, f_imag = gabor(ic, frequency=f)\n",
    "    #print(f_real.shape, f_real)\n",
    "    return [f(f_real.flatten()) for f in funcs]\n",
    "\n",
    "def feat_canny(img, channel, funcs=[np.count_nonzero]):\n",
    "    ic = img[:,:,channel] if len(img.shape) > 2 else img\n",
    "    c = canny(ic)\n",
    "    #print(c.shape, c)\n",
    "    return [f(c) for f in funcs]\n",
    "\n",
    "def feat_hog(img, channel, funcs=[lambda x: x]):\n",
    "    ic = img[:,:,channel] if len(img.shape) > 2 else img\n",
    "    h = hog(ic, block_norm=\"L2-Hys\")\n",
    "    #print(h.shape, h, func(h))\n",
    "    return [f(h) for f in funcs]\n",
    " \n",
    "def feat_all(img):\n",
    "    h, w = feat_imsize(img)\n",
    "    n_ch = img.shape[2] if len(img.shape)>2 else 1\n",
    "    rgavg = feat_chanratio(img, 0, 1)\n",
    "    rbavg = feat_chanratio(img, 0, 2)\n",
    "    gbavg = feat_chanratio(img, 1, 2)\n",
    "    rquadmax = feat_quadrant(img, 0)\n",
    "    gquadmax = feat_quadrant(img, 1)\n",
    "    bquadmax = feat_quadrant(img, 2)\n",
    "    rquadmin = feat_quadrant(img, 0, np.min)\n",
    "    gquadmin = feat_quadrant(img, 1, np.min)\n",
    "    bquadmin = feat_quadrant(img, 2, np.min)\n",
    "    gabor_funcs = [np.count_nonzero, stats.kurtosis, stats.skew]\n",
    "    r_gabor, r_gabor_max, r_gabor_3 = feat_gabor(img, 0, 0.4, gabor_funcs)\n",
    "    g_gabor, g_gabor_max, g_gabor_3 = feat_gabor(img, 1, 0.4, gabor_funcs)\n",
    "    b_gabor, b_gabor_max, b_gabor_3 = feat_gabor(img, 2, 0.4, gabor_funcs)\n",
    "    \n",
    "    hist_funcs = [stats.skew, stats.kurtosis, stats.variation]\n",
    "    rskew, rkurt, rvar = feat_histogram(img, 0, hist_funcs)\n",
    "    gskew, gkurt, gvar = feat_histogram(img, 1, hist_funcs)\n",
    "    bskew, bkurt, bvar = feat_histogram(img, 2, hist_funcs)\n",
    "    \n",
    "    hog_funcs = [stats.skew, stats.kurtosis, np.std]\n",
    "    r_hog, r_hog_k, r_hog3 = feat_hog(img, 0, hog_funcs)\n",
    "    g_hog, g_hog_k, g_hog3 = feat_hog(img, 1, hog_funcs)\n",
    "    b_hog, b_hog_k, b_hog3 = feat_hog(img, 2, hog_funcs)\n",
    "    #print (r_hog, r_hog_k, r_hog3)\n",
    "    \n",
    "    canny_funcs = [np.nanmedian, np.nanmean]\n",
    "    r_c, r_c2 = feat_canny(img, 0, canny_funcs)\n",
    "    g_c, g_c2 = feat_canny(img, 1, canny_funcs)\n",
    "    b_c, b_c2 = feat_canny(img, 2, canny_funcs)\n",
    "    #print (r_c, g_c, b_c)\n",
    "    \n",
    "    return np.array([h, w, n_ch, \n",
    "                     rgavg, rbavg, gbavg, \n",
    "                     #rquadmax, bquadmax, gquadmax,\n",
    "                     #rquadmin, gquadmin, bquadmin,\n",
    "                     rkurt, bkurt, gkurt,\n",
    "                     rkurt/gkurt, rkurt/bkurt, gkurt/bkurt,\n",
    "                     rskew, gskew, bskew,\n",
    "                     rvar, bvar, gvar,\n",
    "                     r_gabor/b_gabor, r_gabor/g_gabor, b_gabor/g_gabor,\n",
    "                     r_gabor_max, b_gabor_max, g_gabor_max,\n",
    "                     r_gabor_3, g_gabor_3, b_gabor_3,\n",
    "                     r_hog, g_hog, b_hog,\n",
    "                     #r_hog_k, g_hog_k, b_hog_k,\n",
    "                     r_hog3, g_hog3, b_hog3,\n",
    "                     r_c, g_c, b_c,\n",
    "                     r_c2, b_c2, g_c2\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate features of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_xy(files, all_categories):\n",
    "    \"\"\"Build the features and targets for the image files provided,\n",
    "    where the targets are taken from the folder name and the provided list of categories\"\"\"\n",
    "    n_files = len(files)\n",
    "    eg_feats = feat_all(read_image(files[np.random.randint(n_files)]))\n",
    "    n_feats = len(eg_feats)\n",
    "    print(f\"Will calculate {n_feats} features for {n_files} images.\")# Feature vectors look like:\\n{eg_feats}\")\n",
    "    x = np.empty((n_files, n_feats), dtype=\"float16\")\n",
    "    y = np.zeros(n_files, dtype=\"int\")\n",
    "    for i,f in enumerate(files):\n",
    "        img = read_image(f)\n",
    "        x_i = feat_all(img)\n",
    "        head, tail = os.path.split(f)\n",
    "        _, target = os.path.split(head)\n",
    "        y[i] = all_categories.index(target)\n",
    "        if np.any(np.isinf(x_i)):\n",
    "            print(i, f, x_i)\n",
    "            x_i[np.isinf(x_i)] = 0\n",
    "        x[i, :] = x_i\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will calculate 39 features for 849 images.\n",
      "(849, 39) (849,)\n"
     ]
    }
   ],
   "source": [
    "x, y = build_xy(subset_filenames, all_categories)\n",
    "print(x.shape, y.shape)\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(np.argwhere(np.isinf(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Classifier cross-validation (Question #3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a random forest Classifier. By convention, clf means 'Classifier'\n",
    "clf = RandomForestClassifier(n_jobs=-1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross-validation scores were:\n",
      "[ 0.16842105  0.20903955  0.24418605  0.20625     0.24666667]\n",
      "Mean: 0.21491266276649448.\n",
      "The chance level with 50 categories is 2% so this is 0.1949126627664945 better.\n"
     ]
    }
   ],
   "source": [
    "xval_scores = model_selection.cross_val_score(clf, x, y, cv=5)\n",
    "print(f\"The cross-validation scores were:\\n{xval_scores}\\nMean: {np.mean(xval_scores)}.\\n\\\n",
    "The chance level with 50 categories is 2% so this is {np.mean(xval_scores)-.02} better.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will calculate 39 features for 4244 images.\n"
     ]
    }
   ],
   "source": [
    "xall, yall = build_xy(all_filenames, all_categories)\n",
    "clf.fit(xall, yall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1 36 19 20 25  4  3 12 14 37  9 38 15  6 17  5 24 10 16  8 18 27 23 31\n",
      " 21 22 32 30 13  7 11 28 29 26  2 33 35 34]\n"
     ]
    }
   ],
   "source": [
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the most important features are the image height and width, and the nanmedian of the canny edge-detector of the blue channel of the image. I really think I could have done better with different features, or by not compressing my features down to one number, but I'm out of time. Oh well.\n",
    "\n",
    "Probably should have pandas-ized this too, as it stands you have to look up where the 0, 1, and 2 are and then figure out what those are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_final_classifier(val_dir):\n",
    "    global clf #clf.fit (above) must have been called!\n",
    "    files = glob.glob(f\"{val_dir}/*.jpg\")\n",
    "    xtest, ytest = build_xy(files[::5], all_categories)\n",
    "    preds = clf.predict(xtest).astype(int)\n",
    "    results = (list(zip(files, ytest, preds)))\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    for r in results:\n",
    "        print(f\"{r[0]}\\t{all_categories[r[2]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put validation directory here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will calculate 39 features for 107 images.\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-493-73a8f0ac1e10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_final_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"50_categories/airplanes/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-492-fc1080a571ef>\u001b[0m in \u001b[0;36mrun_final_classifier\u001b[0;34m(val_dir)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{val_dir}/*.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mxtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_categories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"------------------------------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \"\"\"\n\u001b[0;32m--> 536\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0mcorresponds\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         \"\"\"\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'estimators_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
     ]
    }
   ],
   "source": [
    "run_final_classifier(\"50_categories/airplanes/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final notes\n",
    "\n",
    "This was a cool assignment! Wish I had spent more time on it, considering how early I started, because I got bogged down in an approach and a couple more refactorings and iterations would have been useful. Also it would have been good to do GridSearchCV to figure out the correct hyperparameters for the classifier. I still feel like I learned a lot though.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
